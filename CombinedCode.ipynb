{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c201945f-47bf-46db-a365-dfbe9f21398d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec5e0c3-62f0-4cc0-9558-e52a4c41a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import math\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn. metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720f84c-2e5d-4d7f-887f-394168f3e7d6",
   "metadata": {},
   "source": [
    "### Read data into DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f61546-5551-4c8d-b5de-65e69f8953c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Column names (15 features + label)\n",
    "headers = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'pred'   # label (<=50K / >50K)\n",
    "]\n",
    "\n",
    "# 2. Read raw data\n",
    "original_train_df = pd.read_csv(\n",
    "    \"census-income.data.csv\",\n",
    "    header=None,\n",
    "    names=headers,\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "original_test_df = pd.read_csv(\n",
    "    \"census-income.test.csv\",\n",
    "    header=None,\n",
    "    names=headers,\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "# Work on copies\n",
    "train_df = original_train_df.copy()\n",
    "test_df = original_test_df.copy()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip whitespace\n",
    "\n",
    "def data_to_str(df):\n",
    "    \"\"\"\n",
    "    Strip leading/trailing spaces from all string (object) columns.\n",
    "    \"\"\"\n",
    "    str_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in str_cols:\n",
    "        df[col] = df[col].str.strip()\n",
    "    return df\n",
    "\n",
    "# Apply to both train and test\n",
    "train_df = data_to_str(train_df)\n",
    "test_df = data_to_str(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove trailing period if present\n",
    "test_df['pred'] = test_df['pred'].str.rstrip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"?\" to missing values (NaN) in both dataframes\n",
    "train_df.replace(\"?\", np.nan, inplace=True)\n",
    "test_df.replace(\"?\", np.nan, inplace=True)\n",
    "\n",
    "# Sanity check: how many missing values per column?\n",
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b0aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before dropping\n",
    "print(\"Train shape before dropping NaN:\", train_df.shape)\n",
    "print(\"Test shape before dropping NaN:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any NaN\n",
    "#train_df = train_df.dropna()\n",
    "#test_df = test_df.dropna()\n",
    "\n",
    "# After dropping\n",
    "#print(\"Train shape after dropping NaN:\", train_df.shape)\n",
    "#print(\"Test shape after dropping NaN:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa1aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numeric columns to integer types\n",
    "\n",
    "def data_to_int32(df):\n",
    "    \"\"\"\n",
    "    Convert known numeric columns to int32 (or Int64 if you want to allow NaN).\n",
    "    \"\"\"\n",
    "    int_cols = [\n",
    "        'age',\n",
    "        'fnlwgt',\n",
    "        'education-num',\n",
    "        'capital-gain',\n",
    "        'capital-loss',\n",
    "        'hours-per-week'\n",
    "    ]\n",
    "    for col in int_cols:\n",
    "        df[col] = df[col].astype('int32')\n",
    "    return df\n",
    "\n",
    "train_df = data_to_int32(train_df)\n",
    "test_df = data_to_int32(test_df)\n",
    "\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f86de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removing duplicate rows\n",
    "\n",
    "print(\"Train duplicates:\", train_df.duplicated().sum())\n",
    "print(\"Test duplicates:\", test_df.duplicated().sum())\n",
    "\n",
    "train_df = train_df.drop_duplicates()\n",
    "#test_df = test_df.drop_duplicates()\n",
    "\n",
    "print(\"Train shape after dedup:\", train_df.shape)\n",
    "#print(\"Test shape after dedup:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d161f31",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d83d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_charts = [\n",
    "    'sex','race','workclass','marital-status','occupation',\n",
    "    'relationship','education','education-num','native-country'\n",
    "]\n",
    "\n",
    "n = len(bar_charts)\n",
    "cols = 3\n",
    "rows = (n + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(bar_charts):\n",
    "    s = train_df[column]\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        axes[i].hist(s.dropna(), bins=20)\n",
    "    else:\n",
    "        counts = s.value_counts(dropna=False)\n",
    "        axes[i].bar(range(len(counts)), counts.values)\n",
    "        axes[i].set_xticks(range(len(counts)))\n",
    "        axes[i].set_xticklabels(counts.index.astype(str), rotation=90)\n",
    "\n",
    "    axes[i].set_title(f\"Distribution of {column}\")\n",
    "    axes[i].set_xlabel(column)\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_charts = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "cols = 2\n",
    "rows = 3\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 4 * rows))\n",
    "axes = axes.flatten()   \n",
    "for i, column in enumerate(hist_charts):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Drop NA values for safety\n",
    "    data = train_df[column].dropna()\n",
    "\n",
    "    ax.hist(data, bins=20, edgecolor='black')\n",
    "    \n",
    "    ax.set_title(f\"Histogram of {column}\")\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "# Remove ANY unused subplot axes entirely\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46177cf",
   "metadata": {},
   "source": [
    "## Recategorize the Categorical features into more meaningful groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae99fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "workclass_mapping = {\n",
    "    'State-gov':'Government',\n",
    "    'Local-gov':'Government',\n",
    "    'Federal-gov':'Government',\n",
    "    'Self-emp-inc':'Entrepreneur',\n",
    "    'Self-emp-not-inc':'Entrepreneur',\n",
    "    'Without-pay':'Unemployed',\n",
    "    'Never-worked':'Unemployed',\n",
    "    'Private':'Private',\n",
    "    np.nan:'Unknown'\n",
    "}\n",
    "\n",
    "# Apply mapping and insert into dataframe\n",
    "train_df.insert(2,'workclass-cat',train_df['workclass'].map(workclass_mapping))\n",
    "\n",
    "edu_mapping = {\n",
    "    'Preschool':'HS-dropout',\n",
    "    '1st-4th':'HS-dropout',\n",
    "    '5th-6th':'HS-dropout',\n",
    "    '7th-8th':'HS-dropout',\n",
    "    '9th':'HS-dropout',\n",
    "    '10th':'HS-dropout',\n",
    "    '11th':'HS-dropout',\n",
    "    '12th':'HS-dropout',\n",
    "    'HS-grad':'HS-grad',\n",
    "    'Some-college':'Some-college',\n",
    "    'Assoc-acdm':'Some-college',\n",
    "    'Assoc-voc':'Some-college',\n",
    "    'Bachelors':'Bachelors',\n",
    "    'Masters':'Advanced-degree',\n",
    "    'Prof-school':'Advanced-degree',\n",
    "    'Doctorate':'Advanced-degree',\n",
    "    np.nan:'Unknown'\n",
    "}\n",
    "\n",
    "train_df.insert(4,\"education-cat\",train_df['education'].map(edu_mapping))\n",
    "\n",
    "marital_mapping = {\n",
    "    'Never-married':'Single/Unmarried',\n",
    "    'Divorced':'Single/Unmarried',\n",
    "    'Separated':'Single/Unmarried',\n",
    "    'Widowed':'Single/Unmarried',\n",
    "    'Married-spouse-absent':'Single/Unmarried',\n",
    "    'Married-civ-spouse':'Married',\n",
    "    'Married-AF-spouse':'Married',\n",
    "    np.nan:'Unknown'\n",
    "}\n",
    "\n",
    "train_df.insert(7,'marital-cat',train_df['marital-status'].map(marital_mapping))\n",
    "\n",
    "occupation_mapping = {\n",
    "    'Exec-managerial':'White-collar',\n",
    "    'Prof-specialty':'White-collar',\n",
    "    'Tech-support':'White-collar',\n",
    "    \n",
    "    'Other-service':'Service',\n",
    "    'Sales':'Service',\n",
    "    'Adm-clerical':'Service',\n",
    "    'Protective-serv':'Service',\n",
    "    \n",
    "    'Craft-repair':'Blue-collar',\n",
    "    'Transport-moving':'Blue-collar',\n",
    "    'Machine-op-inspct':'Blue-collar',\n",
    "\n",
    "    'Armed-Forces':'Military',\n",
    "\n",
    "    'Priv-house-serv':'Manual',\n",
    "    'Farming-fishing':'Manual',\n",
    "    'Handlers-cleaners':'Manual',\n",
    "\n",
    "    np.nan:'Unknown'\n",
    "}\n",
    "\n",
    "train_df.insert(10,'occupation-cat',train_df['occupation'].map(occupation_mapping))\n",
    "\n",
    "train_df.insert(18,'native_imm_cat',pd.Series(np.where(train_df['native-country'] == 'United-States', 'Native', 'Immigrant')))\n",
    "\n",
    "# drop all adjusted categorical features in favor of their derived categories\n",
    "train_df.drop(['workclass','fnlwgt','education','education-num','marital-status','occupation','native-country'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7148e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6d18b",
   "metadata": {},
   "source": [
    "## One Hot Encoding and Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OHE for numerical features only\n",
    "\n",
    "# Select numeric columns only\n",
    "numeric_df = train_df.select_dtypes(include=['int32', 'int64', 'float64'])\n",
    "\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Numeric Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OHE for correlation matrix\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoded_df = pd.get_dummies(train_df.drop(columns=['pred']), drop_first=True)\n",
    "\n",
    "# Add encoded label for correlation study\n",
    "encoded_df['target'] = train_df['pred'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "\n",
    "corr = encoded_df.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Full Correlation Matrix (After One-Hot Encoding)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639139cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pairs of correlations\n",
    "corr_pairs = corr.unstack()\n",
    "\n",
    "# Sort by absolute value, descending\n",
    "corr_pairs_sorted = corr_pairs.abs().sort_values(ascending=False)\n",
    "corr_pairs_sorted = corr_pairs_sorted[corr_pairs_sorted < 0.999]  \n",
    "\n",
    "corr_pairs_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pred to binary\n",
    "train_df['target'] = train_df['pred'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "\n",
    "# One-hot encode ALL features except target\n",
    "encoded = pd.get_dummies(train_df.drop(columns=['pred']), drop_first=True)\n",
    "\n",
    "# Compute correlation with target only\n",
    "corr_target = encoded.corr()['target'].sort_values(ascending=False)\n",
    "\n",
    "corr_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,10))\n",
    "sns.heatmap(corr_target.to_frame(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation of Each Feature with Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove native-country\n",
    "encoded_filtered = encoded.drop(columns=[col for col in encoded.columns \n",
    "                                         if col.startswith(\"native-country_\")])\n",
    "\n",
    "# Remove numeric columns\n",
    "numeric_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain',\n",
    "                'capital-loss', 'hours-per-week', 'target']  \n",
    "encoded_filtered = encoded_filtered.drop(columns=[col for col in numeric_cols \n",
    "                                                  if col in encoded_filtered.columns])\n",
    "\n",
    "# Compute correlation with target only (SORTED DESCENDING)\n",
    "corr_target_only = encoded_filtered.join(train_df['target']) \\\n",
    "                                   .corr()['target'] \\\n",
    "                                   .sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdfe731",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 20))\n",
    "sns.heatmap(corr_target_only.to_frame(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            vmin=-0.4,\n",
    "            vmax=0.45)\n",
    "plt.title(\"Categorical Feature Correlations With Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full correlation matrix among remaining categorical columns\n",
    "cat_corr_matrix = encoded_filtered.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9207609",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 20))\n",
    "sns.heatmap(cat_corr_matrix, cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix of Categorical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03349dc",
   "metadata": {},
   "source": [
    "## Normalization --- Jieun's Part\n",
    "\n",
    "capital-gain and capital-loss variables are extremely right-skewed with heavy zeros and a few large outliers. Standard normalization (like Minâ€“Max or Z-score) will not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8adc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log transformation (best for heavy right-skew)\n",
    "log_train_df = train_df.copy()\n",
    "\n",
    "log_train_df['capital_gain_log'] = np.log1p(log_train_df['capital-gain'])\n",
    "log_train_df['capital_loss_log'] = np.log1p(log_train_df['capital-loss'])\n",
    "\n",
    "#log1p(x) handles zeros safely.\n",
    "#Compresses extreme values.\n",
    "#Spreads out dense low-value regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d57da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#capital gain\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(log_train_df['capital_gain_log'], bins=50)\n",
    "plt.title(\"Capital_gain_log\")\n",
    "plt.xlabel(\"capital_gain_log\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "#capital loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(log_train_df['capital_loss_log'], bins=50)\n",
    "plt.title(\"Capital_loss_log\")\n",
    "plt.xlabel(\"capital_loss_log\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d70d0e-bbfa-4efe-be0e-b0111934418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab6ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning\n",
    "bin_train_df = train_df.copy()\n",
    "\n",
    "# capital-gain bins\n",
    "gain_bins = [-1, 0, 5000, 15000, 30000, np.inf]\n",
    "gain_labels = ['no_gain', 'low_gain', 'medium_gain', 'high_gain', 'very_high_gain']\n",
    "\n",
    "bin_train_df['capital_gain_bin'] = pd.cut(\n",
    "    bin_train_df['capital-gain'], \n",
    "    bins=gain_bins, \n",
    "    labels=gain_labels\n",
    ")\n",
    "\n",
    "# capital-loss bins\n",
    "loss_bins = [-1, 0, 1000, 2000, np.inf]\n",
    "loss_labels = ['no_loss', 'low_loss', 'medium_loss', 'high_loss']\n",
    "\n",
    "bin_train_df['capital_loss_bin'] = pd.cut(\n",
    "    bin_train_df['capital-loss'],\n",
    "    bins=loss_bins,\n",
    "    labels=loss_labels\n",
    ")\n",
    "\n",
    "bin_train_df.drop(['capital-gain','capital-loss'],axis=1, inplace=True)\n",
    "\n",
    "print(bin_train_df['capital_gain_bin'].value_counts(), \"\\n\\n\")\n",
    "print(bin_train_df['capital_loss_bin'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c42c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "bin_train_df['capital_gain_bin'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title(\"Capital Gain Bin Counts\")\n",
    "plt.xlabel(\"Capital Gain Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "bin_train_df['capital_loss_bin'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title(\"Capital Loss Bin Counts\")\n",
    "plt.xlabel(\"Capital Loss Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89a646-bbb2-43e3-bc7f-b70c78ad7e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97484b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Indicators\n",
    "binary_train_df = train_df.copy()\n",
    "\n",
    "binary_train_df['has_capital_gain'] = (binary_train_df['capital-gain'] > 0).astype(int)\n",
    "binary_train_df['has_capital_loss'] = (binary_train_df['capital-loss'] > 0).astype(int)\n",
    "\n",
    "binary_train_df.drop(['capital-gain','capital-loss'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2def3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_train_df['has_capital_gain'].value_counts().sort_index().plot(\n",
    "    kind='bar',\n",
    "    figsize=(6,4),\n",
    "    rot=0\n",
    ")\n",
    "plt.title('Capital_gain')\n",
    "plt.xlabel('Capital_gain')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "binary_train_df['has_capital_loss'].value_counts().sort_index().plot(\n",
    "    kind='bar',\n",
    "    figsize=(6,4),\n",
    "    rot=0\n",
    ")\n",
    "plt.title('Capital_loss')\n",
    "plt.xlabel('Capital_loss')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5391b-d5de-4326-bc3a-184e6a1dad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2079951",
   "metadata": {},
   "source": [
    "## Shelsy's Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a353df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampling\n",
    "X_train = train_df.drop ('pred', axis=1)\n",
    "Y_train = train_df['pred']\n",
    "\n",
    "under= RandomUnderSampler(random_state=42)\n",
    "X_under, Y_under = under.fit_resample(X_train, Y_train)\n",
    "\n",
    "print ('before:' , Y_train.value_counts())\n",
    "print ('after:' , Y_under.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ecf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampling\n",
    "over = RandomOverSampler(random_state=42)\n",
    "X_over, Y_over = over.fit_resample(X_train, Y_train)\n",
    "print ('before:' , Y_train.value_counts())\n",
    "print ('after:' , Y_over.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb06829-a32b-47ea-aa24-fb9dbde8370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['capital-gain'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae2460-838b-4c72-9389-4b0ea0472548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
