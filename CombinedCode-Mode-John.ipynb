{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c201945f-47bf-46db-a365-dfbe9f21398d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec5e0c3-62f0-4cc0-9558-e52a4c41a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import math\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline as skPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn. metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720f84c-2e5d-4d7f-887f-394168f3e7d6",
   "metadata": {},
   "source": [
    "### Read data into DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f61546-5551-4c8d-b5de-65e69f8953c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Column names (15 features + label)\n",
    "headers = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'pred'   # label (<=50K / >50K)\n",
    "]\n",
    "\n",
    "# 2. Read raw data\n",
    "original_train_df = pd.read_csv(\n",
    "    \"census-income.data.csv\",\n",
    "    header=None,\n",
    "    names=headers,\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "original_test_df = pd.read_csv(\n",
    "    \"census-income.test.csv\",\n",
    "    header=None,\n",
    "    names=headers,\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "# Work on copies\n",
    "train_df = original_train_df.copy()\n",
    "test_df = original_test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip whitespace\n",
    "\n",
    "def data_to_str(df):\n",
    "    \"\"\"\n",
    "    Strip leading/trailing spaces from all string (object) columns.\n",
    "    \"\"\"\n",
    "    str_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in str_cols:\n",
    "        df[col] = df[col].str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_whitespace(df):\n",
    "    \n",
    "    # Remove trailing period if present\n",
    "    df['pred'] = df['pred'].str.rstrip('.')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_qmarks(df):\n",
    "    \n",
    "    # Convert \"?\" to missing values (NaN) in both dataframes\n",
    "    df.replace(\"?\", np.nan, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa1aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numeric columns to integer types\n",
    "\n",
    "def data_to_int32(df):\n",
    "    \"\"\"\n",
    "    Convert known numeric columns to int32 (or Int64 if you want to allow NaN).\n",
    "    \"\"\"\n",
    "    int_cols = [\n",
    "        'age',\n",
    "        'fnlwgt',\n",
    "        'education-num',\n",
    "        'capital-gain',\n",
    "        'capital-loss',\n",
    "        'hours-per-week'\n",
    "    ]\n",
    "    for col in int_cols:\n",
    "        df[col] = df[col].astype('int32')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f396d-8e06-40b9-89a3-67bd67392535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "\n",
    "    df = data_to_str(df)\n",
    "    df = strip_whitespace(df)\n",
    "    df = replace_qmarks(df)\n",
    "    df = data_to_int32(df)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc44a63-b92c-430c-b95b-249e5a609f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocessing(train_df)\n",
    "test_df = preprocessing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661dc580-9ef2-400b-a7a6-601516e9b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df = train_df.copy()\n",
    "\n",
    "chart_df['pred'] = np.where(chart_df['pred'] == '>50K',1,0)\n",
    "\n",
    "chart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677bf305-ae69-4ab4-92d4-cac70ff884ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prevalence_subplots(df, categories, pred_col='pred'):\n",
    "    n = len(categories)\n",
    "    rows = (n + 2) // 3          # auto-fit 3 plots per row\n",
    "    cols = min(n, 3)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 4 * rows))\n",
    "    axes = axes.flatten() if n > 1 else [axes]\n",
    "\n",
    "    for i, cat in enumerate(categories):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        prevalence = df.groupby(cat)[pred_col].mean()\n",
    "\n",
    "        prevalence.plot(kind='bar', ax=ax)\n",
    "\n",
    "        ax.set_title(f\"Target prevalence by '{cat}' group\")\n",
    "        ax.set_ylabel(\"Proportion with target = 1\")\n",
    "        ax.set_xlabel(cat)\n",
    "\n",
    "    # Hide unused subplots (if any)\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df7ea9-6dd1-4d61-a218-783c3bc1befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['workclass','education','marital-status','occupation','relationship','hours-per-week']\n",
    "\n",
    "plot_prevalence_subplots(chart_df, cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46177cf",
   "metadata": {},
   "source": [
    "## Recategorize the Categorical features into more meaningful groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a574176-2070-4dc4-8f11-24e1f1946d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_remapping(df):\n",
    "\n",
    "    copy_df = df.copy()\n",
    "    \n",
    "    workclass_mapping = {\n",
    "        'State-gov':'Government',\n",
    "        'Local-gov':'Government',\n",
    "        'Federal-gov':'Government',\n",
    "        'Self-emp-inc':'Incorporated-Entrepreneur',\n",
    "        'Self-emp-not-inc':'Unincorporated-Entrepreneur',\n",
    "        'Without-pay':'Unemployed',\n",
    "        'Never-worked':'Unemployed',\n",
    "        'Private':'Private'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping and insert into dataframe\n",
    "    copy_df.insert(2,'workclass-cat',copy_df['workclass'].map(workclass_mapping))\n",
    "    \n",
    "    edu_mapping = {\n",
    "        'Preschool':'HS-dropout',\n",
    "        '1st-4th':'HS-dropout',\n",
    "        '5th-6th':'HS-dropout',\n",
    "        '7th-8th':'HS-dropout',\n",
    "        '9th':'HS-dropout',\n",
    "        '10th':'HS-dropout',\n",
    "        '11th':'HS-dropout',\n",
    "        '12th':'HS-dropout',\n",
    "        'HS-grad':'HS-grad',\n",
    "        'Some-college':'Some-college',\n",
    "        'Assoc-acdm':'Some-college',\n",
    "        'Assoc-voc':'Some-college',\n",
    "        'Bachelors':'Bachelors',\n",
    "        'Masters':'Advanced-degree',\n",
    "        'Prof-school':'Advanced-degree',\n",
    "        'Doctorate':'Advanced-degree'\n",
    "    }\n",
    "    \n",
    "    copy_df.insert(4,\"education-cat\",copy_df['education'].map(edu_mapping))\n",
    "    \n",
    "    marital_mapping = {\n",
    "        'Never-married':'Single/Unmarried',\n",
    "        'Divorced':'Single/Unmarried',\n",
    "        'Separated':'Single/Unmarried',\n",
    "        'Widowed':'Single/Unmarried',\n",
    "        'Married-spouse-absent':'Single/Unmarried',\n",
    "        'Married-civ-spouse':'Married',\n",
    "        'Married-AF-spouse':'Married'\n",
    "    }\n",
    "    \n",
    "    copy_df.insert(7,'marital-cat',copy_df['marital-status'].map(marital_mapping))\n",
    "\n",
    "    occupation_mapping = {\n",
    "        'Exec-managerial':'White-collar',\n",
    "        'Prof-specialty':'White-collar',\n",
    "        'Tech-support':'White-collar',\n",
    "        \n",
    "        'Other-service':'Service',\n",
    "        'Sales':'Service',\n",
    "        'Adm-clerical':'Service',\n",
    "        'Protective-serv':'Service',\n",
    "        \n",
    "        'Craft-repair':'Blue-collar',\n",
    "        'Transport-moving':'Blue-collar',\n",
    "        'Machine-op-inspct':'Blue-collar',\n",
    "    \n",
    "        'Armed-Forces':'Military',\n",
    "    \n",
    "        'Priv-house-serv':'Manual',\n",
    "        'Farming-fishing':'Manual',\n",
    "        'Handlers-cleaners':'Manual'\n",
    "    }\n",
    "    \n",
    "    copy_df.insert(10,'occupation-cat',copy_df['occupation'].map(occupation_mapping))\n",
    "\n",
    "    s = copy_df['native-country']\n",
    "    \n",
    "    native_imm_cat = (\n",
    "        s.map({'United-States': 'Native'})         # US → Native, others → NaN\n",
    "         .fillna('Immigrant')                     # non-US, non-missing → Immigrant\n",
    "         .where(s.notna(), pd.NA)                 # where original was missing, keep NA\n",
    "    )\n",
    "    \n",
    "    native_imm_cat.replace({pd.NA:np.nan}, inplace = True)\n",
    "    \n",
    "    copy_df.insert(18, 'native_imm_cat', native_imm_cat)\n",
    "\n",
    "    hrs_bins = [0, 30, 40, 60, 100]\n",
    "    hrs_labels = ['Part-Time', 'Underworked', 'Full-Time+', 'Overworked']\n",
    "    \n",
    "    copy_df['hours_bin'] = pd.cut(\n",
    "        copy_df['hours-per-week'], \n",
    "        bins=hrs_bins, \n",
    "        labels=hrs_labels\n",
    "    )\n",
    "\n",
    "    # drop all adjusted categorical features in favor of their derived categories\n",
    "    copy_df.drop(['workclass','fnlwgt','education','education-num','marital-status','occupation','native-country','hours-per-week'], axis = 1, inplace = True)\n",
    "\n",
    "    return copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063ceac-4fbd-4456-96df-7da5551a9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = category_remapping(train_df)\n",
    "test_df = category_remapping(test_df)\n",
    "\n",
    "chart_df = train_df.copy()\n",
    "\n",
    "chart_df['pred'] = np.where(chart_df['pred'] == '>50K',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef6ea7-44fc-449b-9b82-0be796070ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['workclass-cat','education-cat','marital-cat','occupation-cat','native_imm_cat','hours_bin']\n",
    "\n",
    "plot_prevalence_subplots(chart_df, cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35144550-5f15-483e-88c9-82dadbc7c60d",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e8d62-30b5-43b4-8728-22be9814d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfb4de-fe84-44c2-bd4f-1a123a1acc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350196c-c8df-41c0-8648-94e1293f0977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns that have missing values\n",
    "columns = {'workclass-cat', 'occupation-cat', 'native_imm_cat'}\n",
    "columns_test = {'workclass-cat', 'occupation-cat', 'native_imm_cat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5777e-3e1b-488a-ab6f-bac07cc2d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print only the columns that contain NA values\n",
    "for col in columns:\n",
    "    if train_df[col].isna().any():\n",
    "        print(f\"\\n{col}\")\n",
    "        print(train_df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8edb1-61ed-44a0-940c-3b8fe628cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TRAIN\n",
    "#Imputation: mode - 2\n",
    "train_df_mode = train_df.copy()\n",
    "\n",
    "for col in columns:\n",
    "    train_df_mode[col] = train_df_mode[col].fillna(train_df_mode[col].mode()[0])\n",
    "\n",
    "train_df_mode.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b1898-8d49-42fc-be11-3e1990606032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEST\n",
    "#Imputation: mode - 2\n",
    "test_df_mode = test_df.copy()\n",
    "\n",
    "for col in columns_test:\n",
    "    test_df_mode[col] = test_df_mode[col].fillna(test_df_mode[col].mode()[0])\n",
    "\n",
    "test_df_mode.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2aed8-08c0-4931-a7a9-5070501eb9c6",
   "metadata": {},
   "source": [
    "## Correlational Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OHE for numerical features only\n",
    "\n",
    "# Select numeric columns only\n",
    "#numeric_df = train_df.select_dtypes(include=['int32', 'int64', 'float64'])\n",
    "numeric_df = train_df_mode.select_dtypes(include=['int32', 'int64', 'float64'])\n",
    "\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Numeric Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OHE for correlation matrix\n",
    "\n",
    "# One-hot encode categorical features\n",
    "#encoded_df = pd.get_dummies(train_df.drop(columns=['pred']), drop_first=True)\n",
    "encoded_df = pd.get_dummies(train_df_mode.drop(columns=['pred']), drop_first=True)\n",
    "\n",
    "# Add encoded label for correlation study\n",
    "#encoded_df['target'] = train_df['pred'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "encoded_df['target'] = train_df_mode['pred'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "\n",
    "corr = encoded_df.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Full Correlation Matrix (After One-Hot Encoding)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pred to binary\n",
    "#binary_corr_df = train_df.copy()\n",
    "binary_corr_df = train_df_mode.copy()\n",
    "binary_corr_df['target'] = binary_corr_df['pred'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "\n",
    "# One-hot encode ALL features except target\n",
    "encoded = pd.get_dummies(binary_corr_df.drop(columns=['pred']), drop_first=True)\n",
    "\n",
    "# Compute correlation with target only\n",
    "corr_target = encoded.corr()['target'].sort_values(ascending=False)\n",
    "\n",
    "corr_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,10))\n",
    "sns.heatmap(corr_target.to_frame(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation of Each Feature with Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove native-country\n",
    "encoded_filtered = encoded.drop(columns=[col for col in encoded.columns \n",
    "                                         if col.startswith(\"native-country_\")])\n",
    "\n",
    "# Remove numeric columns\n",
    "numeric_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain',\n",
    "                'capital-loss', 'target']  \n",
    "encoded_filtered = encoded_filtered.drop(columns=[col for col in numeric_cols \n",
    "                                                  if col in encoded_filtered.columns])\n",
    "\n",
    "# Compute correlation with target only (SORTED DESCENDING)\n",
    "corr_target_only = encoded_filtered.join(binary_corr_df['target']) \\\n",
    "                                   .corr()['target'] \\\n",
    "                                   .sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdfe731",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 20))\n",
    "sns.heatmap(corr_target_only.to_frame(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            vmin=-0.4,\n",
    "            vmax=0.45)\n",
    "plt.title(\"Categorical Feature Correlations With Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full correlation matrix among remaining categorical columns\n",
    "cat_corr_matrix = encoded_filtered.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9207609",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 20))\n",
    "sns.heatmap(cat_corr_matrix, cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix of Categorical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03349dc",
   "metadata": {},
   "source": [
    "## Normalization --- Jieun's Part\n",
    "\n",
    "capital-gain and capital-loss variables are extremely right-skewed with heavy zeros and a few large outliers. Standard normalization (like Min–Max or Z-score) will not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0439895-0146-47b1-86c8-4ddeae4469fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8adc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log transformation (best for heavy right-skew)\n",
    "#log_train_df = train_df.copy()\n",
    "log_train_df = df_mode.copy()\n",
    "\n",
    "log_train_df['capital_gain_log'] = np.log1p(log_train_df['capital-gain'])\n",
    "log_train_df['capital_loss_log'] = np.log1p(log_train_df['capital-loss'])\n",
    "\n",
    "#log1p(x) handles zeros safely.\n",
    "#Compresses extreme values.\n",
    "#Spreads out dense low-value regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b86127-b95d-4067-9fee-f177e271fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##for the testing data\n",
    "\n",
    "#log_train_df = train_df.copy()\n",
    "log_test_df = test_df_mode.copy()\n",
    "\n",
    "log_test_df['capital_gain_log'] = np.log1p(log_test_df['capital-gain'])\n",
    "log_test_df['capital_loss_log'] = np.log1p(log_test_df['capital-loss'])\n",
    "\n",
    "\n",
    "log_test_df\n",
    "print(log_test_df['capital_gain_log'].min())\n",
    "print(log_test_df['capital_gain_log'].max())\n",
    "print(log_train_df['capital_gain_log'].min())\n",
    "print(log_train_df['capital_gain_log'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d57da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#capital gain\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(log_train_df['capital_gain_log'], bins=5)\n",
    "plt.title(\"Capital_gain_log\")\n",
    "plt.xlabel(\"capital_gain_log\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "#capital loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(log_train_df['capital_loss_log'], bins=5)\n",
    "plt.title(\"Capital_loss_log\")\n",
    "plt.xlabel(\"capital_loss_log\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d70d0e-bbfa-4efe-be0e-b0111934418a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5797e0-be89-438b-a4e1-aff47ea0385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= log_train_df.drop (['pred', 'capital-gain','capital-loss'],axis=1)\n",
    "Y_train= log_train_df['pred']\n",
    "\n",
    "X_test = log_test_df.drop(['pred', 'capital-gain','capital-loss'], axis=1)\n",
    "Y_test = log_test_df['pred']\n",
    "#X_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62eecf8-c5a9-4672-bf5f-24ab56305f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
    "print(X_train.columns.tolist())\n",
    "print(X_test.columns.tolist())\n",
    "print(train_df['hours_bin'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe03120-ecbd-4fdc-a95a-42355fbaa466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "imbalances = {\n",
    "    \"none\" : \"passthrough\",\n",
    "    \"under\" : RandomUnderSampler(random_state=42),\n",
    "    \"over\" : RandomOverSampler(random_state=42)\n",
    "}\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "], remainder=\"passthrough\")\n",
    "\n",
    "models = {\n",
    "    \"Decision Tree\" : DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest Classifier\" : RandomForestClassifier(random_state=42),\n",
    "    \"Logistic Regression\" : LogisticRegression(max_iter=5000,solver=\"lbfgs\"),\n",
    "    \"Bagged Decision Tree\": BalancedBaggingClassifier(\n",
    "        estimator = DecisionTreeClassifier(random_state=42), ## can make more for each \n",
    "        n_estimators= 50, \n",
    "        sampling_strategy='auto', \n",
    "        replacement = False, \n",
    "        random_state=42,\n",
    "        n_jobs=1\n",
    "    ),\n",
    "    \"Bagged Random Forest\": BalancedBaggingClassifier(\n",
    "        estimator = RandomForestClassifier(random_state=42), ## can make more for each \n",
    "        n_estimators= 50, \n",
    "        sampling_strategy='auto', \n",
    "        replacement = False, \n",
    "        random_state=42,\n",
    "        n_jobs=1\n",
    "    )\n",
    "}\n",
    "\n",
    "for imbalance_methods, method in imbalances.items():\n",
    "    for model_name, clf in models.items():\n",
    "        if (model_name in [\"Bagged Decision Tree\", \"Bagged Random Forest\"]) and imbalance_methods != \"none\":\n",
    "            continue\n",
    "        print (f\"\\nImbalance Method: {imbalance_methods} \\nModel: {model_name} \")\n",
    "\n",
    "        if model_name == \"Bagged Decision Tree\":\n",
    "                   pipe = Pipeline(steps=[\n",
    "                       (\"encode\", preprocess),\n",
    "                       (\"model\", clf)\n",
    "                   ])\n",
    "        elif model_name == \"Bagged Random Forest\":\n",
    "                   pipe = Pipeline(steps=[\n",
    "                       (\"encode\", preprocess),\n",
    "                       (\"model\", clf)\n",
    "                   ])\n",
    "        else: \n",
    "                   pipe = Pipeline (steps=[\n",
    "                       (\"encode\", preprocess),\n",
    "                       (\"imbalances\", method),\n",
    "                       (\"model\", clf)\n",
    "                   ])\n",
    "        pipe.fit(X_train, Y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "\n",
    "        print (\"test label distribution:\", np.unique(Y_test, return_counts=True))\n",
    "        print (\"predicted label distribution:\", np.unique (y_pred, return_counts=True))\n",
    "        print (classification_report(Y_test, y_pred))\n",
    "        \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c9c4a-e6c5-43d7-a625-7804523ca466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82d4ee-b914-4d2e-b7f6-9233a6474e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900dc73-2b82-4287-b157-c71d530a58e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dcf461-cc20-4b21-93da-f47a728cf552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19aa2a-faed-414a-b3de-27aae5056cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
