{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad6ce67-976e-4ab9-bfaa-6353c73c6b67",
   "metadata": {},
   "source": [
    "# Data Mining | CISC5790 | Final Project\n",
    "## John Chester | Jieun Jeong | Patricia Angeles | Shelsy Saenz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c75bf-14ba-40e4-8196-119968f69c24",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a00df-656d-44fd-b1c0-15e0bdd0904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, FunctionTransformer, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline as skPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399f5ed-0fd8-47bb-8037-861f028f610f",
   "metadata": {},
   "source": [
    "# Reading & Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b932b7-0e54-417b-9503-594ff35b3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Column names (15 features + label)\n",
    "headers = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'pred'   # label (<=50K / >50K)\n",
    "]\n",
    "\n",
    "# 2. Read raw data\n",
    "original_train_df = pd.read_csv(\n",
    "    \"census-income.data.csv\",\n",
    "    header=None,\n",
    "    names=headers,\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "original_test_df = pd.read_csv(\n",
    "    \"census-income.test.csv\",\n",
    "    header=None,\n",
    "    names=headers,\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "# Work on copies\n",
    "train_df = original_train_df.copy()\n",
    "test_df = original_test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9e345-614f-4e3f-aae4-7124c1adc44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target into binary\n",
    "\n",
    "target_map = {\n",
    "'<=50K': 0,\n",
    "'>50K': 1,\n",
    "'<=50K.': 0,\n",
    "'>50K.': 1\n",
    "}\n",
    "\n",
    "train_df['pred'] = train_df['pred'].str.strip().map(target_map)\n",
    "test_df['pred'] = test_df['pred'].str.strip().map(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c32d8-9d27-47c0-8935-26f635d07871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helper Functions ----------\n",
    "\n",
    "def data_to_str(df):\n",
    "    \"\"\"Strip leading/trailing spaces from all object columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    str_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    for col in str_cols:\n",
    "        df[col] = df[col].str.strip()\n",
    "    return df\n",
    "\n",
    "def replace_qmarks(df):\n",
    "    \"\"\"Convert '?' to np.nan.\"\"\"\n",
    "    return df.replace(\"?\", np.nan)\n",
    "\n",
    "\n",
    "def category_remapping(df):\n",
    "    '''Maps categorical variables into more functional bins'''\n",
    "    copy_df = df.copy()\n",
    "    \n",
    "    # workclass\n",
    "    workclass_mapping = {\n",
    "        'State-gov':'Government',\n",
    "        'Local-gov':'Government',\n",
    "        'Federal-gov':'Government',\n",
    "        'Self-emp-inc':'Incorporated-Entrepreneur',\n",
    "        'Self-emp-not-inc':'Unincorporated-Entrepreneur',\n",
    "        'Without-pay':'Unemployed',\n",
    "        'Never-worked':'Unemployed',\n",
    "        'Private':'Private'\n",
    "    }\n",
    "    if \"workclass\" in copy_df:\n",
    "        copy_df[\"workclass-cat\"] = copy_df[\"workclass\"].map(workclass_mapping)\n",
    "    \n",
    "    # education\n",
    "    edu_mapping = {\n",
    "        'Preschool':'HS-dropout',\n",
    "        '1st-4th':'HS-dropout',\n",
    "        '5th-6th':'HS-dropout',\n",
    "        '7th-8th':'HS-dropout',\n",
    "        '9th':'HS-dropout',\n",
    "        '10th':'HS-dropout',\n",
    "        '11th':'HS-dropout',\n",
    "        '12th':'HS-dropout',\n",
    "        'HS-grad':'HS-grad',\n",
    "        'Some-college':'Some-college',\n",
    "        'Assoc-acdm':'Some-college',\n",
    "        'Assoc-voc':'Some-college',\n",
    "        'Bachelors':'Bachelors',\n",
    "        'Masters':'Advanced-degree',\n",
    "        'Prof-school':'Advanced-degree',\n",
    "        'Doctorate':'Advanced-degree'\n",
    "    }\n",
    "    if \"education\" in copy_df:\n",
    "        copy_df[\"education-cat\"] = copy_df[\"education\"].map(edu_mapping)\n",
    "    \n",
    "    # marital-status\n",
    "    marital_mapping = {\n",
    "        'Never-married':'Single/Unmarried',\n",
    "        'Divorced':'Single/Unmarried',\n",
    "        'Separated':'Single/Unmarried',\n",
    "        'Widowed':'Single/Unmarried',\n",
    "        'Married-spouse-absent':'Single/Unmarried',\n",
    "        'Married-civ-spouse':'Married',\n",
    "        'Married-AF-spouse':'Married'\n",
    "    }\n",
    "    if \"marital-status\" in copy_df:\n",
    "        copy_df[\"marital-cat\"] = copy_df[\"marital-status\"].map(marital_mapping)\n",
    "    \n",
    "    # occupation\n",
    "    occupation_mapping = {\n",
    "        'Exec-managerial':'White-collar',\n",
    "        'Prof-specialty':'White-collar',\n",
    "        'Tech-support':'White-collar',\n",
    "        'Other-service':'Service',\n",
    "        'Sales':'Service',\n",
    "        'Adm-clerical':'Service',\n",
    "        'Protective-serv':'Service',\n",
    "        'Craft-repair':'Blue-collar',\n",
    "        'Transport-moving':'Blue-collar',\n",
    "        'Machine-op-inspct':'Blue-collar',\n",
    "        'Armed-Forces':'Military',\n",
    "        'Priv-house-serv':'Manual',\n",
    "        'Farming-fishing':'Manual',\n",
    "        'Handlers-cleaners':'Manual'\n",
    "    }\n",
    "    if \"occupation\" in copy_df:\n",
    "        copy_df[\"occupation-cat\"] = copy_df[\"occupation\"].map(occupation_mapping)\n",
    "    \n",
    "    # native-country → native_imm_cat\n",
    "    if \"native-country\" in copy_df:\n",
    "        s = copy_df[\"native-country\"]\n",
    "        native_imm_cat = (\n",
    "            s.map({\"United-States\": \"Native\"})  # US → Native, others NaN\n",
    "             .fillna(\"Immigrant\")              # non-US, non-missing → Immigrant\n",
    "             .where(s.notna(), pd.NA)          # where original was missing, keep NA\n",
    "        )\n",
    "        copy_df[\"native_imm_cat\"] = native_imm_cat.replace({pd.NA: np.nan})\n",
    "    \n",
    "    # hours-per-week binning\n",
    "    if \"hours-per-week\" in copy_df:\n",
    "        hrs_bins = [0, 30, 40, 60, 100]\n",
    "        hrs_labels = [\"Part-Time\", \"Underworked\", \"Full-Time+\", \"Overworked\"]\n",
    "        copy_df[\"hours_bin\"] = pd.cut(copy_df[\"hours-per-week\"], bins=hrs_bins, labels=hrs_labels)\n",
    "    \n",
    "    # capital-flow binning\n",
    "    if \"capital-gain\" in copy_df and \"capital-loss\" in copy_df:\n",
    "        copy_df[\"net-capital-flow\"] = copy_df[\"capital-gain\"] - copy_df[\"capital-loss\"]\n",
    "        cap_bins = [-10000, 10000, 99999]\n",
    "        cap_labels = [\"Standard\", \"High Net-Worth\"]\n",
    "        copy_df[\"cap_flow_bin\"] = pd.cut(copy_df[\"net-capital-flow\"], bins=cap_bins, labels=cap_labels)\n",
    "    \n",
    "    # Drop original columns we replaced\n",
    "    drop_cols = [\n",
    "        \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "        \"marital-status\", \"occupation\", \"native-country\",\n",
    "        \"hours-per-week\", \"capital-gain\", \"capital-loss\",\n",
    "        \"net-capital-flow\"\n",
    "    ]\n",
    "    existing = [c for c in drop_cols if c in copy_df.columns]\n",
    "    copy_df = copy_df.drop(columns=existing)\n",
    "    \n",
    "    return copy_df\n",
    "\n",
    "\n",
    "class BasicPrep(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    1) strip whitespace\n",
    "    2) convert '?' → np.nan\n",
    "    3) apply all category/bucket remappings & drop originals\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df = data_to_str(df)\n",
    "        df = replace_qmarks(df)\n",
    "        df = category_remapping(df)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8f95a-01da-459f-bb4e-cffa6dfbec2c",
   "metadata": {},
   "source": [
    "**We dropped fnlwgt as it's not a useful statistic, and we know that education-num covers the same information as education so we drop that as well.**\n",
    "\n",
    "**Based on our analysis further in the notebook, we identify categorical divisions for our features that provide strong relationships to the target variable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a6345-67ec-4c48-90c6-095f3fee6044",
   "metadata": {},
   "source": [
    "**Convert \"?\" to np.nan, strip whitespace from string values, remap categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde85bc1-f098-476e-ba40-70f5d2b51bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BP = BasicPrep()\n",
    "\n",
    "cleaned_df = train_df.copy()\n",
    "\n",
    "cleaned_df = BP.transform(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c048b-a882-494a-983c-17e5468ab6ea",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7db4d-fffe-4f97-abe1-9af0aedae50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b7213-5982-445f-96db-910c17f66f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns that have missing values\n",
    "columns = {'workclass-cat', 'occupation-cat', 'native_imm_cat'}\n",
    "columns_test = {'workclass-cat', 'occupation-cat', 'native_imm_cat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e47784-f13f-4741-bc65-bab07450ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print only the columns that contain NA values\n",
    "for col in columns:\n",
    "    if cleaned_df[col].isna().any():\n",
    "        print(f\"\\n{col}\")\n",
    "        print(cleaned_df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b10ce-c1c9-412e-8a1e-119843576588",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TRAIN\n",
    "#Imputation: mode - 2\n",
    "cleaned_df_mode = cleaned_df.copy()\n",
    "\n",
    "for col in columns:\n",
    "    cleaned_df_mode[col] = cleaned_df_mode[col].fillna(cleaned_df_mode[col].mode()[0])\n",
    "\n",
    "cleaned_df_mode.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9f56f-750c-40e6-a847-10a8916d2497",
   "metadata": {},
   "source": [
    "**Our dataset is cleaned**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936ed06-cf03-42d2-90d4-e2b00ded598f",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e22bc-a2c9-4b78-9308-9607a7fc2e2a",
   "metadata": {},
   "source": [
    "## Correlation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce5be0-ae78-475c-91c2-6e70b1c5aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OHE for numeric features only\n",
    "\n",
    "# Select numeric columns only\n",
    "#numeric_df = train_df.select_dtypes(include=['int32', 'int64', 'float64'])\n",
    "numeric_df = cleaned_df_mode.select_dtypes(include=['int32', 'int64', 'float64'])\n",
    "\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap=\"crest\",             \n",
    "    linewidths=0.6,\n",
    "    linecolor=\"white\",\n",
    "    annot_kws={\"fontsize\": 14, \"color\": \"white\"}  \n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    \"Correlation Matrix of Numeric Features\",\n",
    "    fontsize=26,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#0B2D2F\",\n",
    "    pad=20\n",
    ")\n",
    "\n",
    "plt.xticks(\n",
    "    fontsize=14,\n",
    "    rotation=45,\n",
    "    ha=\"right\",\n",
    "    color=\"#0B2D2F\"\n",
    ")\n",
    "\n",
    "plt.yticks(\n",
    "    fontsize=14,\n",
    "    rotation=0,\n",
    "    color=\"#0B2D2F\"\n",
    ")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b685a38-a20b-4090-8c90-6b000be8aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pred to binary\n",
    "#binary_corr_df = train_df.copy()\n",
    "binary_corr_df = cleaned_df_mode.copy()\n",
    "\n",
    "# One-hot encode ALL features except target\n",
    "encoded = pd.get_dummies(binary_corr_df, drop_first=True)\n",
    "\n",
    "# Compute correlation with target only\n",
    "corr_target = encoded.corr()['pred'].sort_values(ascending=False)\n",
    "\n",
    "corr_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3bdd4-75ca-4efa-9021-2f87a6ad7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 15))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    corr_target.to_frame(),\n",
    "    annot=True,\n",
    "    cmap='crest',\n",
    "    vmin=-0.4,\n",
    "    vmax=0.45,\n",
    "    linewidths=0.5,\n",
    "    annot_kws={\"fontsize\": 14, \"color\": \"white\"}  # bigger, darker numbers\n",
    ")\n",
    "\n",
    "plt.xticks(fontsize=18, rotation=0, color=\"#0B2D2F\")\n",
    "plt.yticks(fontsize=16, rotation=0, color=\"#0B2D2F\")\n",
    "\n",
    "plt.title(\n",
    "    \"Correlation of Each Feature with Target\",\n",
    "    fontsize=28,\n",
    "    fontweight='bold',\n",
    "    color=\"#0B2D2F\",\n",
    "    pad=20\n",
    ")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1fa0ed-413f-4112-bc54-2a7cdaeb4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prevalence_individual(df, categories, pred_col='pred'):\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    teal_gradient = sns.color_palette(\"crest\", 20) \n",
    "\n",
    "    for cat in categories:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        prevalence = df.groupby(cat)[pred_col].mean().sort_values()\n",
    "\n",
    "        colors = teal_gradient[:len(prevalence)]\n",
    "\n",
    "        ax = prevalence.plot(\n",
    "            kind='bar',\n",
    "            color=colors,\n",
    "            edgecolor=\"black\"\n",
    "        )\n",
    "\n",
    "        plt.title(\n",
    "            f\"Target Prevalence by {cat.replace('-', ' ').title()}\",\n",
    "            fontsize=20,\n",
    "            fontweight='bold',\n",
    "            color=\"#0B2D2F\",            \n",
    "            pad=15\n",
    "        )\n",
    "        plt.ylabel(\"Proportion with target = 1\", fontsize=14, color=\"#0B2D2F\")\n",
    "        plt.xlabel(cat.replace('-', ' ').title(), fontsize=14, color=\"#0B2D2F\")\n",
    "\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=14, color=\"#0B2D2F\")\n",
    "        plt.yticks(fontsize=14, color=\"#0B2D2F\")\n",
    "\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "        sns.despine()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1695c345-a08c-410e-aee0-d1d7405621fc",
   "metadata": {},
   "source": [
    "### Visualizing Target Prevalence by Feature\n",
    "**This will inform our decisions regarding remapping and binning these specified categories into broader categories with more meaning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed5474-42be-4610-8416-c4c430fda5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['workclass','education','marital-status','occupation','relationship','hours-per-week']\n",
    "plot_prevalence_individual(train_df, cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd617309-64d2-4a1a-a6fb-624ea4c96b28",
   "metadata": {},
   "source": [
    "**Now we apply these transformations to the data - remappings and binnings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f402d84-00df-4088-ab43-dad635e02795",
   "metadata": {},
   "outputs": [],
   "source": [
    "BP = BasicPrep()\n",
    "\n",
    "copy_df = train_df.copy()\n",
    "\n",
    "eda_df = BP.transform(copy_df)\n",
    "\n",
    "my_col = eda_df.pop('pred')\n",
    "\n",
    "eda_df['pred'] = my_col\n",
    "\n",
    "eda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0b9a2-ea20-4305-a116-3c5cd8da0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['workclass-cat','education-cat','marital-cat','occupation-cat','hours_bin','cap_flow_bin']\n",
    "plot_prevalence_individual(eda_df, cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8adb1e-1c19-45bc-b90d-25f381cab031",
   "metadata": {},
   "source": [
    "**These charts show fairly effective groupings were made - there is clear stratification between these groupings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85e9da-1308-42da-a628-7edd3a502a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cat_target_bars(df, cat_col, y_label = 'Category', target_col=\"pred\"):\n",
    "    \"\"\"\n",
    "    Diverging horizontal bar chart of correlation between each category of\n",
    "    cat_col and the target variable.\n",
    "\n",
    "    y-axis label  = y_label (e.g. \"Education\")\n",
    "    y-tick labels = clean category names (e.g. \"Bachelors\", \"HS-grad\", ...)\n",
    "    \"\"\"\n",
    "    sns.set_theme(style = 'white')\n",
    "    # 1) One-hot encode the category\n",
    "    dummies = pd.get_dummies(df[cat_col], prefix=cat_col, drop_first=False)\n",
    "\n",
    "    # 2) Correlation with target\n",
    "    corr_df = pd.concat([dummies, df[target_col]], axis=1)\n",
    "    corr = corr_df.corr()[target_col].drop(target_col)\n",
    "\n",
    "    # 3) Build Series of clean labels (indexed by dummy column name)\n",
    "    clean_labels = pd.Series(\n",
    "        corr.index.str.replace(f\"{cat_col}_\", \"\", regex=False),\n",
    "        index=corr.index\n",
    "    )\n",
    "\n",
    "    # 4) Sort correlations (strongest positive first; change if you prefer abs)\n",
    "    corr_sorted = corr.sort_values(ascending=False)\n",
    "    clean_labels_sorted = clean_labels.loc[corr_sorted.index]\n",
    "\n",
    "    # 5) Colors (green for +, blue-gray for −)\n",
    "    colors = corr_sorted.apply(lambda x: \"#A3C9A8\" if x > 0 else \"#87AFC7\")\n",
    "\n",
    "    # 6) Positions for bars\n",
    "    y_pos = np.arange(len(corr_sorted))\n",
    "\n",
    "    plt.figure(figsize=(9, 3))\n",
    "\n",
    "    # Bars\n",
    "    plt.barh(y_pos, corr_sorted.values, color=colors)\n",
    "\n",
    "    # y-tick labels = clean category names\n",
    "    plt.yticks(y_pos, clean_labels_sorted.values)\n",
    "\n",
    "    # Zero line\n",
    "    plt.axvline(0, color=\"black\", linewidth=1)\n",
    "\n",
    "    # Annotate values on bars\n",
    "\n",
    "\n",
    "    plt.title(\"Correlation of Categories vs Target (+/- $50K)\", fontsize=14)\n",
    "    plt.xlabel(\"Correlation\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xlim(-0.5, 0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e673b-6068-4696-b7df-d87e2e96212c",
   "metadata": {},
   "source": [
    "**Here we provide another view of these categories, broken out by column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a84bf9-d822-4fca-846f-c5231f0304d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_target_bars(eda_df, 'education-cat', 'Education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217410d7-ccc7-4efd-b6ce-17cc36bcb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_target_bars(eda_df, 'occupation-cat', 'Occupation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b02f7-9bf6-4273-9ecd-f70b431f959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_target_bars(eda_df, 'workclass-cat', 'Work Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730a0a9-d517-4bac-91f1-e44ce8f1a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_target_bars(eda_df, 'marital-cat', 'Marital Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09febe02-734f-47a4-b5dc-91a7b4cec25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_target_bars(eda_df, 'native_imm_cat', 'Native/Immigrant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b73ed-6b5d-4c53-8f78-93ed115f8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_target_bars(eda_df, 'hours_bin', 'Hours Worked per Week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e905a-964c-479f-b2de-8777b5c16943",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_target_bars(eda_df, 'cap_flow_bin', 'Indiv. NW Status')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3402293-a8b6-4c2f-980c-bfb1bf7d7c29",
   "metadata": {},
   "source": [
    "### Additional Correlation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d07cc4-de8a-4453-8a61-7294e6589d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Encode categoricals, keep pred numeric ---\n",
    "# (drop_first=False so every category appears)\n",
    "encoded_df = pd.get_dummies(\n",
    "    eda_df.drop(columns=[\"pred\"]),\n",
    "    drop_first=False\n",
    ")\n",
    "\n",
    "# add target back\n",
    "encoded_df[\"pred\"] = eda_df[\"pred\"].values\n",
    "\n",
    "# correlation matrix\n",
    "corr_matrix = encoded_df.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(11, 11))  # put BEFORE heatmap\n",
    "sns.set_theme(style = 'white')\n",
    "ax = sns.heatmap(\n",
    "    corr_matrix,\n",
    "    mask=mask,\n",
    "    cmap=sns.diverging_palette(225, 130, as_cmap=True),\n",
    "    linewidths=0.5,\n",
    "    center=0\n",
    ")\n",
    "\n",
    "ax.set_title(\"Correlation Matrix\", fontsize=18, pad=20)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c9ce9d-6deb-4c59-a762-cd45852163f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute correlation with target\n",
    "target_corr = corr_matrix[\"pred\"].abs()\n",
    "\n",
    "thresh = 0.2\n",
    "\n",
    "# choose \"important\" columns\n",
    "important_cols = target_corr[target_corr > thresh].index\n",
    "\n",
    "# filter matrix to those cols only\n",
    "filtered_corr = corr_matrix.loc[important_cols, important_cols]\n",
    "\n",
    "mask_filt = np.triu(np.ones_like(filtered_corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "ax = sns.heatmap(\n",
    "    filtered_corr,\n",
    "    mask=mask_filt,\n",
    "    cmap=sns.diverging_palette(225, 130, as_cmap=True),\n",
    "    linewidths=1,\n",
    "    center=0,\n",
    "    annot=False\n",
    ")\n",
    "\n",
    "ax.set_title(f\"Filtered Correlation Matrix (|corr with pred| > {thresh})\", fontsize=16, pad=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e17c20-91f6-47a2-8d52-df3446bc0dab",
   "metadata": {},
   "source": [
    "# Preparing, Running, and Tuning Models with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da1e89-47b2-44ab-a2db-f4b47bb68a78",
   "metadata": {},
   "source": [
    "### Prepare Missing-Drop Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a9438-f15c-4d04-b38d-12a3df843b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_drop = train_df.copy()\n",
    "\n",
    "train_df_drop = data_to_str(train_df_drop)\n",
    "train_df_drop = replace_qmarks(train_df_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0947ac0f-52d5-4f02-8bf2-97ae5078e80a",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e182d3-a400-47de-a23c-424db9a35531",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"pred\"\n",
    "\n",
    "X_train_raw = train_df.drop(columns=[target_col])\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_test_raw  = test_df.drop(columns=[target_col])\n",
    "y_test  = test_df[target_col]\n",
    "\n",
    "X_train_drop_raw = train_df_drop.drop(columns=[target_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c559c6-2a6b-4b0c-a293-07c740e5c84f",
   "metadata": {},
   "source": [
    "# Testing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22e7a3-1249-4584-88b0-7b164b866911",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Build transformers, identify features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32d097-d30f-4d6d-862c-2f11766f2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessor\n",
    "\n",
    "numeric_features = [\"age\"]\n",
    "\n",
    "categorical_features = [\n",
    "    \"relationship\", \"race\", \"sex\",\n",
    "    \"workclass-cat\", \"education-cat\", \"marital-cat\",\n",
    "    \"occupation-cat\", \"native_imm_cat\",\n",
    "    \"hours_bin\", \"cap_flow_bin\"\n",
    "]\n",
    "\n",
    "# Numeric pipeline\n",
    "numeric_transformer = skPipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log\", FunctionTransformer(np.log1p, feature_names_out=\"one-to-one\")),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_transformer = skPipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(\n",
    "        strategy=\"most_frequent\",   # or \"constant\" with fill_value=\"Unknown\"\n",
    "        fill_value=\"Unknown\"\n",
    "    )),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f425b-92c8-4967-8298-f4282d67ad32",
   "metadata": {},
   "source": [
    "## Build pipeline & test using GridSearchCV\n",
    "### Testing OverSampling vs No Imbalancing Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18933ed3-badb-45da-933a-b677c746187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_base = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    solver=\"lbfgs\",\n",
    "    penalty=\"l2\"\n",
    ")\n",
    "\n",
    "logreg_pipeline = ImbPipeline(steps=[\n",
    "    (\"prep\", BasicPrep()),\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"sampler\", \"passthrough\"),  # will be set to ROS or passthrough in grid\n",
    "    (\"clf\", logreg_base)\n",
    "])\n",
    "\n",
    "logreg_param_grid = {\n",
    "    # --- categorical missing-data ---\n",
    "    \"preprocessor__cat__imputer__strategy\": [\"most_frequent\", \"constant\"], # Constant = fill value with \"Unknown\"\n",
    "\n",
    "    # --- numeric normalization on age ---\n",
    "    \"preprocessor__num__log\": [\n",
    "        \"passthrough\",\n",
    "        FunctionTransformer(np.log1p, feature_names_out=\"one-to-one\") # Log Transformation\n",
    "    ],\n",
    "    \"preprocessor__num__scaler\": [\n",
    "        \"passthrough\",\n",
    "        MinMaxScaler() # MinMaxScaler\n",
    "    ],\n",
    "\n",
    "    # --- imbalance handling ---\n",
    "    \"sampler\": [\n",
    "        \"passthrough\", # None\n",
    "        RandomOverSampler(random_state=42) # Random Oversampling\n",
    "    ],\n",
    "\n",
    "    # --- LogisticRegression hyperparams ---\n",
    "    \"clf__C\": [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "logreg_grid = GridSearchCV(\n",
    "    estimator=logreg_pipeline,\n",
    "    param_grid=logreg_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",  # or 'f1_macro'\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "logreg_grid.fit(X_train_raw, y_train)\n",
    "\n",
    "print(\"Best params (LogReg):\", logreg_grid.best_params_)\n",
    "print(\"Best CV accuracy (LogReg):\", logreg_grid.best_score_)\n",
    "\n",
    "print(\"Test accuracy (LogReg):\", logreg_grid.score(X_test_raw, y_test))\n",
    "print(classification_report(y_test, logreg_grid.predict(X_test_raw)))\n",
    "\n",
    "y_pred_lr=logreg_grid.predict(X_test_raw)\n",
    "conf= confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf, annot=True, cmap= \"crest\", fmt=\"d\", cbar=False)\n",
    "plt.xlabel('predicted labels')\n",
    "plt.ylabel ('true Labels')\n",
    "plt.title('confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841d0c1-1b0a-4e8d-b7e2-e5b01b76379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr_pipe = logreg_grid.best_estimator_\n",
    "preprocessor_best = best_lr_pipe.named_steps[\"preprocessor\"]\n",
    "lr_clf = best_lr_pipe.named_steps[\"clf\"]\n",
    "\n",
    "# numeric feature names\n",
    "num_transformer = preprocessor_best.named_transformers_[\"num\"]\n",
    "if hasattr(num_transformer, \"get_feature_names_out\"):\n",
    "    feature_names_num = num_transformer.get_feature_names_out(numeric_features)\n",
    "else:\n",
    "    feature_names_num = np.array(numeric_features)\n",
    "\n",
    "# categorical feature names from OneHotEncoder\n",
    "cat_pipeline = preprocessor_best.named_transformers_[\"cat\"]\n",
    "ohe = cat_pipeline.named_steps[\"onehot\"]\n",
    "feature_names_cat = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "all_feature_names = np.concatenate([feature_names_num, feature_names_cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a90a9-4f89-43d1-89fa-6a4855835221",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = lr_clf.coef_[0]  # shape (1, n_features)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": all_feature_names,\n",
    "    \"coef\": coefs,\n",
    "    \"odds_ratio\": np.exp(coefs)\n",
    "})\n",
    "\n",
    "# Largest positive & negative coefficients\n",
    "top_pos = coef_df.sort_values(\"coef\", ascending=False).head(15)\n",
    "top_neg = coef_df.sort_values(\"coef\", ascending=True).head(15)\n",
    "top_coef = pd.concat([top_neg, top_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f06ae-b4d6-4a24-b134-46219d002862",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "top_coef_sorted = top_coef.sort_values(\"coef\")\n",
    "\n",
    "# Generate colors from the \"crest\" colormap\n",
    "cmap = cm.get_cmap(\"crest\")\n",
    "colors = cmap(np.linspace(0, 1, len(top_coef_sorted)))\n",
    "\n",
    "# Create bar chart with colormap colors\n",
    "plt.barh(top_coef_sorted[\"feature\"], top_coef_sorted[\"coef\"], color=colors)\n",
    "\n",
    "plt.axvline(0, color=\"black\", linewidth=1)\n",
    "plt.title(\"Logistic Regression – Top Positive and Negative Coefficients\")\n",
    "plt.xlabel(\"Coefficient (log-odds)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ed6a2d-25c1-47cf-a38c-6671ae6d50ea",
   "metadata": {},
   "source": [
    "## RandomForestClassifier\n",
    "### Build transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47440a31-0cf3-463a-a95d-6d25a6185f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer_rf = skPipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "    # no log, no scaler – RF is tree-based and scale-invariant\n",
    "])\n",
    "\n",
    "preprocessor_rf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer_rf, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5e0e2-cf9e-45e6-9bed-1575747dfb30",
   "metadata": {},
   "source": [
    "### Build pipeline & test using GridSearchCV - Test with and without OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa03be1-7bce-442b-b4d8-36731bb2c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_pipeline = ImbPipeline(steps=[\n",
    "    (\"prep\", BasicPrep()),\n",
    "    (\"preprocessor\", preprocessor_rf),\n",
    "    (\"sampler\", \"passthrough\"),\n",
    "    (\"clf\", rf_base)\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    # cat missing data\n",
    "    \"preprocessor__cat__imputer__strategy\": [\"most_frequent\", \"constant\"],\n",
    "\n",
    "    # imbalance handling\n",
    "    \"sampler\": [\n",
    "        \"passthrough\",\n",
    "        RandomOverSampler(random_state=42)\n",
    "    ],\n",
    "\n",
    "    # RF hyperparams\n",
    "    \"clf__n_estimators\": [100, 200], \n",
    "    \"clf__max_depth\": [None, 20],\n",
    "    \"clf__min_samples_leaf\": [2]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train_raw, y_train)\n",
    "\n",
    "print(\"Best params (RF):\", rf_grid.best_params_)\n",
    "print(\"Best CV accuracy (RF):\", rf_grid.best_score_)\n",
    "\n",
    "print(\"Test accuracy (RF):\", rf_grid.score(X_test_raw, y_test))\n",
    "print(classification_report(y_test, rf_grid.predict(X_test_raw)))\n",
    "\n",
    "y_pred_rf=rf_grid.predict(X_test_raw)\n",
    "\n",
    "conf= confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf, annot=True, cmap= \"crest\", fmt=\"d\", cbar=False)\n",
    "plt.xlabel('predicted labels')\n",
    "plt.ylabel ('true Labels')\n",
    "plt.title('confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e187a-2369-4a6a-ac29-c17e2bf81e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = rf_grid.best_estimator_.named_steps[\"clf\"]\n",
    "importances = best_rf.feature_importances_\n",
    "\n",
    "feature_names = rf_grid.best_estimator_.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "fi = pd.Series(importances, index=feature_names).sort_values(ascending=False).head(15)\n",
    "fi.plot(kind=\"barh\", figsize=(8,6),color= sns.color_palette(\"YlGnBu\", n_colors=15)[::-1])\n",
    "plt.title(\"15 Most Important Features- Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614e1133-c338-4af9-9834-4973af292d6a",
   "metadata": {},
   "source": [
    "### Testing another Imbalance Correction - Balanced Subsample Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0aaad-6f76-4c12-9ec8-dd02d83eef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"age\"]\n",
    "\n",
    "categorical_features = [\n",
    "    \"relationship\", \"race\", \"sex\",\n",
    "    \"workclass-cat\", \"education-cat\", \"marital-cat\",\n",
    "    \"occupation-cat\", \"native_imm_cat\",\n",
    "    \"hours_bin\", \"cap_flow_bin\"\n",
    "]\n",
    "\n",
    "numeric_transformer_rf = skPipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))   # no scaling/log for RF\n",
    "])\n",
    "\n",
    "categorical_transformer = skPipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor_rf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer_rf, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e82c0e-ba86-4925-a310-a41c36434bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base RF settings (same for both)\n",
    "rf_common_kwargs = dict(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 1) RF + balanced_subsample\n",
    "rf_balanced = RandomForestClassifier(\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    **rf_common_kwargs\n",
    ")\n",
    "\n",
    "rf_balanced_pipe = ImbPipeline(steps=[\n",
    "    (\"prep\", BasicPrep()),\n",
    "    (\"preprocessor\", preprocessor_rf),\n",
    "    (\"sampler\", \"passthrough\"),\n",
    "    (\"clf\", rf_balanced)\n",
    "])\n",
    "\n",
    "rf_os = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_pipeline = ImbPipeline(steps=[\n",
    "    (\"prep\", BasicPrep()),\n",
    "    (\"preprocessor\", preprocessor_rf),\n",
    "    (\"sampler\", \"passthrough\"),\n",
    "    (\"clf\", rf_base)\n",
    "])\n",
    "\n",
    "rf_os_param_grid = {\n",
    "    # cat missing data\n",
    "    \"preprocessor__cat__imputer__strategy\": [\"most_frequent\", \"constant\"],\n",
    "\n",
    "    # imbalance handling\n",
    "    \"sampler\": [\n",
    "        \"passthrough\",\n",
    "        RandomOverSampler(random_state=42)\n",
    "    ],\n",
    "\n",
    "    # RF hyperparams\n",
    "    \"clf__n_estimators\": [100, 200], \n",
    "    \"clf__max_depth\": [None, 20],\n",
    "    \"clf__min_samples_leaf\": [2]\n",
    "}\n",
    "\n",
    "rf_os_grid = GridSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#2) RF + OverSampling\n",
    "\n",
    "\n",
    "# 3) RF + no rebalancing\n",
    "rf_plain = RandomForestClassifier(\n",
    "    class_weight=None,   # no balancing\n",
    "    **rf_common_kwargs\n",
    ")\n",
    "\n",
    "rf_plain_pipe = ImbPipeline(steps=[\n",
    "    (\"prep\", BasicPrep()),         # cleaning + remapping\n",
    "    (\"preprocessor\", preprocessor_rf),\n",
    "    (\"sampler\", \"passthrough\"),    # no oversampling\n",
    "    (\"clf\", rf_plain)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9450c72-7de1-43b9-a399-d84f20591b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RF (Plain) ===\")\n",
    "rf_plain_pipe.fit(X_train_raw, y_train)\n",
    "y_pred_plain = rf_plain_pipe.predict(X_test_raw)\n",
    "print(classification_report(y_test, y_pred_plain))\n",
    "\n",
    "print(\"\\n=== RF + balanced_subsample ===\")\n",
    "rf_balanced_pipe.fit(X_train_raw, y_train)\n",
    "y_pred_bal = rf_balanced_pipe.predict(X_test_raw)\n",
    "print(classification_report(y_test, y_pred_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925a06f-e66f-443c-b1fd-cd25a18c0e82",
   "metadata": {},
   "source": [
    "## Imbalancing\n",
    "\n",
    "### LogReg - Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d6eb28-2197-4859-8432-aa9d545a91bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_base = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1,\n",
    "    solver=\"lbfgs\",\n",
    "    penalty=\"l2\",\n",
    "    class_weight=None   # we’ll let bagging do the balancing\n",
    ")\n",
    "\n",
    "bb_logreg = BalancedBaggingClassifier(\n",
    "    estimator=lr_base,\n",
    "    n_estimators=25,          # you can bump to 50+ if runtime is OK\n",
    "    sampling_strategy=\"auto\", # balance the classes in each bag\n",
    "    replacement=True,         # bootstrap sampling\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "bb_logreg_pipe = ImbPipeline(steps=[\n",
    "    (\"prep\", BasicPrep()),     # your cleaner + remapper\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"clf\", bb_logreg)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8217d04e-278a-4768-8dce-9b34a7f7783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Balanced Bagging + Logistic Regression ===\")\n",
    "bb_logreg_pipe.fit(X_train_raw, y_train)\n",
    "y_pred_bb = bb_logreg_pipe.predict(X_test_raw)\n",
    "print(classification_report(y_test, y_pred_bb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b35797-8dae-4ee8-8666-955ee048adbe",
   "metadata": {},
   "source": [
    "### LogReg - Bagging - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12eaaf5-145e-47e6-9194-231a6d358d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Bagged Logistic Regression base setup -----\n",
    "lr_base = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1,\n",
    "    solver=\"lbfgs\",\n",
    "    penalty=\"l2\"\n",
    ")\n",
    "\n",
    "bb_logreg = BalancedBaggingClassifier(\n",
    "    estimator=lr_base,\n",
    "    n_estimators=50,          \n",
    "    sampling_strategy=\"auto\", # class balancing per bag\n",
    "    replacement=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "bb_logreg_pipe = ImbPipeline(steps=[\n",
    "    (\"prep\", BasicPrep()),     # your cleaning + remapping\n",
    "    (\"preprocessor\", preprocessor),  # log + MinMax + OHE\n",
    "    (\"clf\", bb_logreg)\n",
    "])\n",
    "\n",
    "# ----- Hyperparameter grid for Bagged LogReg -----\n",
    "lr_param_grid = {\n",
    "    \"clf__n_estimators\": [200],                 # number of bags\n",
    "    \"clf__estimator__C\": [0.1, 1.0, 10.0],         # LR regularization\n",
    "    \"clf__estimator__class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    estimator=bb_logreg_pipe,\n",
    "    param_grid=lr_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1_macro\",  # or 'accuracy'\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ----- Fit + evaluate -----\n",
    "lr_grid.fit(X_train_raw, y_train)\n",
    "\n",
    "print(\"Best params (Bagged LogReg):\", lr_grid.best_params_)\n",
    "print(\"Best CV score (Bagged LogReg):\", lr_grid.best_score_)\n",
    "\n",
    "y_pred_lr = lr_grid.predict(X_test_raw)\n",
    "print(\"\\n=== Test performance: Bagged LogReg ===\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26597bf0-aacf-4ade-abe0-663030d95b90",
   "metadata": {},
   "source": [
    "# RandomForest / Decision Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187ccb1-70ae-4ea1-b0ab-68d7e5aad5b0",
   "metadata": {},
   "source": [
    "### Decision Tree - Bagging - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8afcc-18c1-4782-bee9-9101af1688b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Bagged Decision Tree base setup -----\n",
    "base_tree = DecisionTreeClassifier(\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bb_tree = BalancedBaggingClassifier(\n",
    "    estimator=base_tree,\n",
    "    n_estimators=25,          # will be tuned\n",
    "    sampling_strategy=\"auto\", # balance classes in each bag\n",
    "    replacement=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "dt_bag_pipe = ImbPipeline(steps=[\n",
    "    (\"prep\", BasicPrep()),     # your cleaning + remapping\n",
    "    (\"preprocessor\", preprocessor_rf),\n",
    "    (\"clf\", bb_tree)\n",
    "])\n",
    "\n",
    "# ----- Hyperparameter grid for Bagged DT -----\n",
    "dt_param_grid = {\n",
    "    \"clf__n_estimators\": [200, 400],          # size of the bagging ensemble\n",
    "    \"clf__estimator__max_depth\": [None, 10, 20],\n",
    "    \"clf__estimator__min_samples_leaf\": [1, 2, 5]\n",
    "}\n",
    "\n",
    "dt_grid = GridSearchCV(\n",
    "    estimator=dt_bag_pipe,\n",
    "    param_grid=dt_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1_macro\",  # or \"accuracy\", but f1_macro is nicer for imbalance\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ----- Fit + evaluate -----\n",
    "dt_grid.fit(X_train_raw, y_train)\n",
    "\n",
    "print(\"Best params (Bagged DT):\", dt_grid.best_params_)\n",
    "print(\"Best CV score (Bagged DT):\", dt_grid.best_score_)\n",
    "\n",
    "y_pred_dt = dt_grid.predict(X_test_raw)\n",
    "print(\"\\n=== Test performance: Bagged DT ===\")\n",
    "print(classification_report(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a0552-d4f5-4b94-833c-ceac1ace4933",
   "metadata": {},
   "source": [
    "## Decision Tree - Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1ccf0-3721-4a0b-a66a-8c1d68187702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base tree for each bag\n",
    "base_tree = DecisionTreeClassifier(\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bb_clf = BalancedBaggingClassifier(\n",
    "    estimator=base_tree,           # base estimator in each bag\n",
    "    n_estimators=100,               # number of bags (ensemble size)\n",
    "    sampling_strategy=\"auto\",      # balance minority/majority automatically\n",
    "    replacement=True,              # bootstrap sampling\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "bb_pipe = ImbPipeline(steps=[\n",
    "    (\"prep\", BasicPrep()),         # your cleaning + remapping\n",
    "    (\"preprocessor\", preprocessor_rf),\n",
    "    (\"clf\", bb_clf)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5281b91-f23d-48da-b55d-140abe37d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RF_plain\": rf_plain_pipe,\n",
    "    \"RF_balanced_subsample\": rf_balanced_pipe,\n",
    "    \"BalancedBagging_DT\": bb_pipe\n",
    "}\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    pipe.fit(X_train_raw, y_train)\n",
    "    y_pred = pipe.predict(X_test_raw)\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf57cb-bb8a-47e0-bb30-14bbb5220f49",
   "metadata": {},
   "source": [
    "### Missing Data - Drop Method - RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffb7c9-5c39-4e35-b0a8-38bf8b5da462",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    # cat missing data\n",
    "    #\"preprocessor__cat__imputer__strategy\": [\"most_frequent\", \"constant\"],\n",
    "\n",
    "    # imbalance handling\n",
    "    \"sampler\": [\n",
    "        \"passthrough\",\n",
    "        RandomOverSampler(random_state=42)\n",
    "    ],\n",
    "\n",
    "    # RF hyperparams – trimmed\n",
    "    \"clf__n_estimators\": [200],\n",
    "    \"clf__max_depth\": [20],\n",
    "    \"clf__min_samples_leaf\": [2]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train_drop_raw, y_train)\n",
    "\n",
    "print(\"Best params (RF):\", rf_grid.best_params_)\n",
    "print(\"Best CV accuracy (RF):\", rf_grid.best_score_)\n",
    "\n",
    "print(\"Test accuracy (RF):\", rf_grid.score(X_test_raw, y_test))\n",
    "print(classification_report(y_test, rf_grid.predict(X_test_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe998551-1a35-412a-9712-9816b1d46f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dict reports\n",
    "lr_report = classification_report(y_test, y_pred_lr, output_dict=True)\n",
    "rf_report = classification_report(y_test, y_pred_rf, output_dict=True)\n",
    "\n",
    "# Build DataFrame for plotting\n",
    "metrics_df = pd.DataFrame({\n",
    "    (\"Logistic Regression\", \"F1\"):    [lr_report[\"0\"][\"f1-score\"], lr_report[\"1\"][\"f1-score\"]],\n",
    "    (\"Random Forest\",        \"F1\"):    [rf_report[\"0\"][\"f1-score\"], rf_report[\"1\"][\"f1-score\"]],\n",
    "    (\"Logistic Regression\", \"Recall\"): [lr_report[\"0\"][\"recall\"],   lr_report[\"1\"][\"recall\"]],\n",
    "    (\"Random Forest\",        \"Recall\"): [rf_report[\"0\"][\"recall\"],   rf_report[\"1\"][\"recall\"]],\n",
    "}, index=[\"<=50K\", \">50K\"])\n",
    "\n",
    "metrics_df.columns = pd.MultiIndex.from_tuples(metrics_df.columns)\n",
    "\n",
    "# Plot F1 only (simpler) or both\n",
    "f1_df = metrics_df.xs(\"F1\", axis=1, level=1)\n",
    "\n",
    "f1_df.plot(kind=\"bar\", cmap= \"crest\")\n",
    "plt.title(\"F1-score by Class: Logistic Regression vs Random Forest\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# If you also want Recall:\n",
    "recall_df = metrics_df.xs(\"Recall\", axis=1, level=1)\n",
    "recall_df.plot(kind=\"bar\", cmap= \"crest\")\n",
    "plt.title(\"Recall by Class: Logistic Regression vs Random Forest\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
