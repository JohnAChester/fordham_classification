{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c201945f-47bf-46db-a365-dfbe9f21398d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec5e0c3-62f0-4cc0-9558-e52a4c41a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline as skPipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720f84c-2e5d-4d7f-887f-394168f3e7d6",
   "metadata": {},
   "source": [
    "# Read data into DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f61546-5551-4c8d-b5de-65e69f8953c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Column names (15 features + label)\n",
    "headers = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'pred'   # label (<=50K / >50K)\n",
    "]\n",
    "\n",
    "# 2. Read raw data\n",
    "original_train_df = pd.read_csv(\n",
    "    \"census-income.data.csv\",\n",
    "    header=None,\n",
    "    names=headers,\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "original_test_df = pd.read_csv(\n",
    "    \"census-income.test.csv\",\n",
    "    header=None,\n",
    "    names=headers,\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "# Work on copies\n",
    "train_df = original_train_df.copy()\n",
    "test_df = original_test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb41f80-a0d5-47b7-8224-04d10547d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target into binary\n",
    "\n",
    "target_map = {\n",
    "'<=50K': 0,\n",
    "'>50K': 1,\n",
    "'<=50K.': 0,\n",
    "'>50K.': 1\n",
    "}\n",
    "\n",
    "train_df['pred'] = train_df['pred'].str.strip().map(target_map)\n",
    "test_df['pred'] = test_df['pred'].str.strip().map(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cca0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helper Functions ----------\n",
    "\n",
    "def data_to_str(df):\n",
    "    \"\"\"Strip leading/trailing spaces from all object columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    str_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    for col in str_cols:\n",
    "        df[col] = df[col].str.strip()\n",
    "    return df\n",
    "\n",
    "def replace_qmarks(df):\n",
    "    \"\"\"Convert '?' to np.nan.\"\"\"\n",
    "    return df.replace(\"?\", np.nan)\n",
    "\n",
    "\n",
    "def category_remapping(df):\n",
    "    '''Maps categorical variables into more functional bins'''\n",
    "    copy_df = df.copy()\n",
    "    \n",
    "    # workclass\n",
    "    workclass_mapping = {\n",
    "        'State-gov':'Government',\n",
    "        'Local-gov':'Government',\n",
    "        'Federal-gov':'Government',\n",
    "        'Self-emp-inc':'Incorporated-Entrepreneur',\n",
    "        'Self-emp-not-inc':'Unincorporated-Entrepreneur',\n",
    "        'Without-pay':'Unemployed',\n",
    "        'Never-worked':'Unemployed',\n",
    "        'Private':'Private'\n",
    "    }\n",
    "    if \"workclass\" in copy_df:\n",
    "        copy_df[\"workclass-cat\"] = copy_df[\"workclass\"].map(workclass_mapping)\n",
    "    \n",
    "    # education\n",
    "    edu_mapping = {\n",
    "        'Preschool':'HS-dropout',\n",
    "        '1st-4th':'HS-dropout',\n",
    "        '5th-6th':'HS-dropout',\n",
    "        '7th-8th':'HS-dropout',\n",
    "        '9th':'HS-dropout',\n",
    "        '10th':'HS-dropout',\n",
    "        '11th':'HS-dropout',\n",
    "        '12th':'HS-dropout',\n",
    "        'HS-grad':'HS-grad',\n",
    "        'Some-college':'Some-college',\n",
    "        'Assoc-acdm':'Some-college',\n",
    "        'Assoc-voc':'Some-college',\n",
    "        'Bachelors':'Bachelors',\n",
    "        'Masters':'Advanced-degree',\n",
    "        'Prof-school':'Advanced-degree',\n",
    "        'Doctorate':'Advanced-degree'\n",
    "    }\n",
    "    if \"education\" in copy_df:\n",
    "        copy_df[\"education-cat\"] = copy_df[\"education\"].map(edu_mapping)\n",
    "    \n",
    "    # marital-status\n",
    "    marital_mapping = {\n",
    "        'Never-married':'Single/Unmarried',\n",
    "        'Divorced':'Single/Unmarried',\n",
    "        'Separated':'Single/Unmarried',\n",
    "        'Widowed':'Single/Unmarried',\n",
    "        'Married-spouse-absent':'Single/Unmarried',\n",
    "        'Married-civ-spouse':'Married',\n",
    "        'Married-AF-spouse':'Married'\n",
    "    }\n",
    "    if \"marital-status\" in copy_df:\n",
    "        copy_df[\"marital-cat\"] = copy_df[\"marital-status\"].map(marital_mapping)\n",
    "    \n",
    "    # occupation\n",
    "    occupation_mapping = {\n",
    "        'Exec-managerial':'White-collar',\n",
    "        'Prof-specialty':'White-collar',\n",
    "        'Tech-support':'White-collar',\n",
    "        'Other-service':'Service',\n",
    "        'Sales':'Service',\n",
    "        'Adm-clerical':'Service',\n",
    "        'Protective-serv':'Service',\n",
    "        'Craft-repair':'Blue-collar',\n",
    "        'Transport-moving':'Blue-collar',\n",
    "        'Machine-op-inspct':'Blue-collar',\n",
    "        'Armed-Forces':'Military',\n",
    "        'Priv-house-serv':'Manual',\n",
    "        'Farming-fishing':'Manual',\n",
    "        'Handlers-cleaners':'Manual'\n",
    "    }\n",
    "    if \"occupation\" in copy_df:\n",
    "        copy_df[\"occupation-cat\"] = copy_df[\"occupation\"].map(occupation_mapping)\n",
    "    \n",
    "    # native-country → native_imm_cat\n",
    "    if \"native-country\" in copy_df:\n",
    "        s = copy_df[\"native-country\"]\n",
    "        native_imm_cat = (\n",
    "            s.map({\"United-States\": \"Native\"})  # US → Native, others NaN\n",
    "             .fillna(\"Immigrant\")              # non-US, non-missing → Immigrant\n",
    "             .where(s.notna(), pd.NA)          # where original was missing, keep NA\n",
    "        )\n",
    "        copy_df[\"native_imm_cat\"] = native_imm_cat.replace({pd.NA: np.nan})\n",
    "    \n",
    "    # hours-per-week binning\n",
    "    if \"hours-per-week\" in copy_df:\n",
    "        hrs_bins = [0, 30, 40, 60, 100]\n",
    "        hrs_labels = [\"Part-Time\", \"Underworked\", \"Full-Time+\", \"Overworked\"]\n",
    "        copy_df[\"hours_bin\"] = pd.cut(copy_df[\"hours-per-week\"], bins=hrs_bins, labels=hrs_labels)\n",
    "    \n",
    "    # capital-flow binning\n",
    "    if \"capital-gain\" in copy_df and \"capital-loss\" in copy_df:\n",
    "        copy_df[\"net-capital-flow\"] = copy_df[\"capital-gain\"] - copy_df[\"capital-loss\"]\n",
    "        cap_bins = [-10000, 10000, 99999]\n",
    "        cap_labels = [\"Standard\", \"High Net-Worth\"]\n",
    "        copy_df[\"cap_flow_bin\"] = pd.cut(copy_df[\"net-capital-flow\"], bins=cap_bins, labels=cap_labels)\n",
    "    \n",
    "    # Drop original columns we replaced\n",
    "    drop_cols = [\n",
    "        \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "        \"marital-status\", \"occupation\", \"native-country\",\n",
    "        \"hours-per-week\", \"capital-gain\", \"capital-loss\",\n",
    "        \"net-capital-flow\"\n",
    "    ]\n",
    "    existing = [c for c in drop_cols if c in copy_df.columns]\n",
    "    copy_df = copy_df.drop(columns=existing)\n",
    "    \n",
    "    return copy_df\n",
    "\n",
    "\n",
    "class BasicPrep(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    1) strip whitespace\n",
    "    2) convert '?' → np.nan\n",
    "    3) apply all category/bucket remappings & drop originals\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df = data_to_str(df)\n",
    "        df = replace_qmarks(df)\n",
    "        df = category_remapping(df)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da8d651a-4139-447c-8947-ce431299b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_drop = train_df.copy()\n",
    "\n",
    "train_df_drop = data_to_str(train_df_drop)\n",
    "train_df_drop = replace_qmarks(train_df_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5fb717-2401-4e97-8cbb-61d8d92dc5ac",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d08698-df58-49f0-8057-ecaf70c51fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"pred\"\n",
    "\n",
    "X_train_raw = train_df.drop(columns=[target_col])\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_test_raw  = test_df.drop(columns=[target_col])\n",
    "y_test  = test_df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94988fdb-5af7-49f1-a6df-fdb648b9d00f",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "## Build transformers, identify features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e89c9c4a-e6c5-43d7-a625-7804523ca466",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"age\"]\n",
    "\n",
    "categorical_features = [\n",
    "    \"relationship\", \"race\", \"sex\",\n",
    "    \"workclass-cat\", \"education-cat\", \"marital-cat\",\n",
    "    \"occupation-cat\", \"native_imm_cat\",\n",
    "    \"hours_bin\", \"cap_flow_bin\"\n",
    "]\n",
    "\n",
    "# Numeric pipeline\n",
    "numeric_transformer = skPipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log\", FunctionTransformer(np.log1p, feature_names_out=\"one-to-one\")),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_transformer = skPipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(\n",
    "        strategy=\"most_frequent\",   # or \"constant\" with fill_value=\"Unknown\"\n",
    "        fill_value=\"Unknown\"\n",
    "    )),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d612db3-93ab-42e5-a3ad-044748291da7",
   "metadata": {},
   "source": [
    "## Build pipeline & test using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f6300ec-0f1d-4bb2-a34c-09bf4f701508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best params (LogReg): {'clf__C': 10.0, 'preprocessor__cat__imputer__strategy': 'constant', 'preprocessor__num__log': FunctionTransformer(feature_names_out='one-to-one', func=<ufunc 'log1p'>), 'preprocessor__num__scaler': MinMaxScaler(), 'sampler': 'passthrough'}\n",
      "Best CV accuracy (LogReg): 0.8424496310723857\n",
      "Test accuracy (LogReg): 0.8436828204655734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90     12435\n",
      "           1       0.71      0.57      0.63      3846\n",
      "\n",
      "    accuracy                           0.84     16281\n",
      "   macro avg       0.79      0.75      0.77     16281\n",
      "weighted avg       0.84      0.84      0.84     16281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_base = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    solver=\"lbfgs\",\n",
    "    penalty=\"l2\"\n",
    ")\n",
    "\n",
    "logreg_pipeline = ImbPipeline(steps=[\n",
    "    (\"prep\", BasicPrep()),\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"sampler\", \"passthrough\"),  # will be set to ROS or passthrough in grid\n",
    "    (\"clf\", logreg_base),\n",
    "])\n",
    "\n",
    "logreg_param_grid = {\n",
    "    # --- categorical missing-data ---\n",
    "    \"preprocessor__cat__imputer__strategy\": [\"most_frequent\", \"constant\"],\n",
    "    # \"constant\" uses fill_value=\"Unknown\" already set above\n",
    "\n",
    "    # --- numeric normalization on age ---\n",
    "    \"preprocessor__num__log\": [\n",
    "        \"passthrough\",\n",
    "        FunctionTransformer(np.log1p, feature_names_out=\"one-to-one\"),\n",
    "    ],\n",
    "    \"preprocessor__num__scaler\": [\n",
    "        \"passthrough\",\n",
    "        MinMaxScaler(),\n",
    "    ],\n",
    "\n",
    "    # --- imbalance handling ---\n",
    "    \"sampler\": [\n",
    "        \"passthrough\",\n",
    "        RandomOverSampler(random_state=42),\n",
    "    ],\n",
    "\n",
    "    # --- LogisticRegression hyperparams ---\n",
    "    \"clf__C\": [0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "logreg_grid = GridSearchCV(\n",
    "    estimator=logreg_pipeline,\n",
    "    param_grid=logreg_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",  # or 'f1_macro'\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "logreg_grid.fit(X_train_raw, y_train)\n",
    "\n",
    "print(\"Best params (LogReg):\", logreg_grid.best_params_)\n",
    "print(\"Best CV accuracy (LogReg):\", logreg_grid.best_score_)\n",
    "\n",
    "print(\"Test accuracy (LogReg):\", logreg_grid.score(X_test_raw, y_test))\n",
    "print(classification_report(y_test, logreg_grid.predict(X_test_raw)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337783bb-6031-4cd4-8d3b-df32de624ffb",
   "metadata": {},
   "source": [
    "# RandomForestClassifier\n",
    "## Build transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b6aec9-2504-4b34-a232-6c2f9bab9ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer_rf = skPipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    # no log, no scaler – RF is tree-based and scale-invariant\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3a0110b-25bf-4764-a6da-2ca13978f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_rf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer_rf, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ce141-73d6-4e4b-9c0c-964566ab4af4",
   "metadata": {},
   "source": [
    "## Build pipeline & test using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c4f5ccc-c5cc-4983-9893-42af248bbf55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "[CV] END clf__max_depth=None, clf__min_samples_leaf=1, clf__n_estimators=100, preprocessor__cat__imputer__strategy=most_frequent, sampler=passthrough; total time=   6.1s\n",
      "[CV] END clf__max_depth=None, clf__min_samples_leaf=1, clf__n_estimators=100, preprocessor__cat__imputer__strategy=constant, sampler=passthrough; total time=   6.0s\n",
      "[CV] END clf__max_depth=None, clf__min_samples_leaf=1, clf__n_estimators=200, preprocessor__cat__imputer__strategy=most_frequent, sampler=RandomOverSampler(random_state=42); total time=  18.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnchester/Desktop/homework/env/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__max_depth=None, clf__min_samples_leaf=1, clf__n_estimators=100, preprocessor__cat__imputer__strategy=most_frequent, sampler=passthrough; total time=   6.4s\n",
      "[CV] END clf__max_depth=None, clf__min_samples_leaf=1, clf__n_estimators=100, preprocessor__cat__imputer__strategy=constant, sampler=RandomOverSampler(random_state=42); total time=   9.6s\n",
      "[CV] END clf__max_depth=None, clf__min_samples_leaf=1, clf__n_estimators=200, preprocessor__cat__imputer__strategy=most_frequent, sampler=RandomOverSampler(random_state=42); total time=  18.0s\n",
      "[CV] END clf__max_depth=None, clf__min_samples_leaf=2, clf__n_estimators=100, preprocessor__cat__imputer__strategy=constant, sampler=passthrough; total time=   4.0s\n",
      "[CV] END clf__max_depth=None, clf__min_samples_leaf=1, clf__n_estimators=100, preprocessor__cat__imputer__strategy=most_frequent, sampler=RandomOverSampler(random_state=42); total time=   9.3s\n",
      "[CV] END clf__max_depth=None, clf__min_samples_leaf=1, clf__n_estimators=200, preprocessor__cat__imputer__strategy=most_frequent, sampler=RandomOverSampler(random_state=42); total time=  19.4s\n",
      "[CV] END clf__max_depth=None, clf__min_samples_leaf=1, clf__n_estimators=200, preprocessor__cat__imputer__strategy=constant, sampler=RandomOverSampler(random_state=42); total time=  16.6s\n",
      "Best params (RF): {'clf__max_depth': 20, 'clf__min_samples_leaf': 2, 'clf__n_estimators': 200, 'preprocessor__cat__imputer__strategy': 'constant', 'sampler': 'passthrough'}\n",
      "Best CV accuracy (RF): 0.8429717205166305\n",
      "Test accuracy (RF): 0.8447884036607088\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90     12435\n",
      "           1       0.71      0.58      0.64      3846\n",
      "\n",
      "    accuracy                           0.84     16281\n",
      "   macro avg       0.79      0.75      0.77     16281\n",
      "weighted avg       0.84      0.84      0.84     16281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_pipeline = ImbPipeline(steps=[\n",
    "    (\"prep\", BasicPrep()),\n",
    "    (\"preprocessor\", preprocessor_rf),\n",
    "    (\"sampler\", \"passthrough\"),\n",
    "    (\"clf\", rf_base),\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    # cat missing data\n",
    "    \"preprocessor__cat__imputer__strategy\": [\"most_frequent\", \"constant\"],\n",
    "\n",
    "    # imbalance handling\n",
    "    \"sampler\": [\n",
    "        \"passthrough\",\n",
    "        RandomOverSampler(random_state=42),\n",
    "    ],\n",
    "\n",
    "    # RF hyperparams – trimmed\n",
    "    \"clf__n_estimators\": [100, 200],       # 400 is overkill for a grid\n",
    "    \"clf__max_depth\": [None, 20],          # drop the 10\n",
    "    \"clf__min_samples_leaf\": [1, 2],       # drop the 5\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train_raw, y_train)\n",
    "\n",
    "print(\"Best params (RF):\", rf_grid.best_params_)\n",
    "print(\"Best CV accuracy (RF):\", rf_grid.best_score_)\n",
    "\n",
    "print(\"Test accuracy (RF):\", rf_grid.score(X_test_raw, y_test))\n",
    "print(classification_report(y_test, rf_grid.predict(X_test_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6f491-ab81-4d39-8c1c-eb611fae44bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
